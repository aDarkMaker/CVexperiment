# 基于前馈神经网络的分类任务实验报告

## 实验目的
设计一个前馈神经网络，对二维高斯数据进行分类任务，比较不同网络架构、激活函数和学习率对分类性能的影响。

## 实验环境
- 深度学习框架：PyTorch
- 编程语言：Python
- 数据集：dataset.csv（4000个样本，4个类别）

## 数据集描述
- 数据集包含4000个样本，每个样本有2个特征和1个标签
- 标签为1、2、3、4，分别对应4个不同的高斯分布类别
- 数据预处理：标签转换为0、1、2、3，特征标准化
- 数据划分：90%训练集（3600样本），10%测试集（400样本）

## 实验配置
设计了5种不同的神经网络配置进行比较：

### 配置1：单隐藏层64节点+ReLU
- 网络架构：[2 → 64 → 4]
- 激活函数：ReLU
- 学习率：0.001
- 训练轮数：100

### 配置2：双隐藏层128-64节点+ReLU
- 网络架构：[2 → 128 → 64 → 4]
- 激活函数：ReLU
- 学习率：0.001
- 训练轮数：100

### 配置3：单隐藏层32节点+Tanh
- 网络架构：[2 → 32 → 4]
- 激活函数：Tanh
- 学习率：0.001
- 训练轮数：100

### 配置4：单隐藏层64节点+Sigmoid
- 网络架构：[2 → 64 → 4]
- 激活函数：Sigmoid
- 学习率：0.001
- 训练轮数：100

### 配置5：高学习率单隐藏层64节点+ReLU
- 网络架构：[2 → 64 → 4]
- 激活函数：ReLU
- 学习率：0.01
- 训练轮数：100

## 实验结果

### 性能对比表
| 配置 | 训练集准确率 | 测试集准确率 | 网络架构 | 激活函数 | 学习率 |
|------|-------------|-------------|----------|----------|--------|
| 单隐藏层64节点+ReLU | 98.75% | 99.00% | [64] | ReLU | 0.001 |
| 双隐藏层128-64节点+ReLU | 98.81% | 98.75% | [128, 64] | ReLU | 0.001 |
| 单隐藏层32节点+Tanh | 98.67% | 99.25% | [32] | Tanh | 0.001 |
| 单隐藏层64节点+Sigmoid | 98.64% | 99.00% | [64] | Sigmoid | 0.001 |
| 高学习率单隐藏层64节点+ReLU | 98.50% | 99.00% | [64] | ReLU | 0.01 |

### 最佳配置
- **最佳测试准确率**：99.25%
- **最佳配置**：单隐藏层32节点+Tanh
- **网络架构**：[2 → 32 → 4]
- **激活函数**：Tanh
- **学习率**：0.001

## 损失曲线分析

### 训练损失（每个mini-batch）
所有配置的训练损失都呈现快速下降后趋于稳定的趋势：
- 前10个epoch内损失迅速下降
- 后续epoch中损失在0.035-0.045之间波动
- 表明模型能够有效学习数据特征

### 测试损失（每个epoch）
- 测试损失与训练损失趋势一致，表明没有明显的过拟合
- 最终测试损失在0.020-0.030之间
- 双隐藏层配置的测试损失略低，但准确率并非最高

## 准确率曲线分析

### 训练准确率
- 所有配置的训练准确率在98.5%-98.8%之间
- 训练准确率在第10个epoch左右达到98%以上
- 后续训练中准确率保持稳定，表明模型收敛良好

### 测试准确率
- 测试准确率在98.75%-99.25%之间
- 测试准确率与训练准确率相近，表明模型泛化能力强
- 单隐藏层32节点+Tanh配置表现最佳

## 实验分析

### 1. 网络深度影响
- **单隐藏层 vs 双隐藏层**：双隐藏层网络（[128, 64]）在训练集上准确率略高（98.81%），但在测试集上表现略差（98.75%），可能存在轻微过拟合
- **复杂度与泛化**：对于这个相对简单的分类任务，单隐藏层已经足够，增加网络深度带来的性能提升有限

### 2. 激活函数比较
- **ReLU**：训练稳定，收敛快，适合深层网络
- **Tanh**：在本次实验中表现最佳，可能更适合这个特定数据分布
- **Sigmoid**：收敛相对较慢，但最终性能与ReLU相当

### 3. 学习率影响
- **标准学习率（0.001）**：训练稳定，收敛过程平滑
- **高学习率（0.01）**：训练过程略有波动，最终性能与标准学习率相当
- 对于这个任务，学习率在0.001-0.01范围内都能取得良好效果

### 4. 隐藏层节点数
- **32节点**：在测试集上表现最佳（99.25%）
- **64节点**：性能稳定（99.00%）
- **128节点**：在双隐藏层中表现良好
- 表明对于这个任务，适中的节点数（32-64）已经足够

## 结论

1. **模型有效性**：所有配置的前馈神经网络都能有效完成二维高斯数据的分类任务，测试准确率均在98.75%以上

2. **最佳配置**：单隐藏层32节点+Tanh激活函数+0.001学习率组合表现最佳，测试准确率达到99.25%

3. **网络复杂度**：对于这个相对简单的分类任务，单隐藏层已经足够，增加网络深度或节点数带来的性能提升有限

4. **激活函数选择**：Tanh函数在这个任务中表现最佳，可能更适合高斯分布数据的特性

5. **学习率选择**：标准学习率（0.001）表现稳定，高学习率（0.01）虽然收敛更快但训练过程略有波动

6. **泛化能力**：所有配置在训练集和测试集上的性能相近，表明模型具有良好的泛化能力，没有明显的过拟合现象

## 实验局限性
1. 数据集相对简单，所有配置都能取得很高的准确率
2. 实验配置数量有限，可能存在更好的超参数组合
3. 没有进行交叉验证来进一步验证模型稳定性

## 改进方向
1. 尝试更多的网络架构和超参数组合
2. 使用交叉验证评估模型稳定性
3. 添加正则化技术防止过拟合
4. 尝试不同的优化器和学习率调度策略
